{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetic-gpu8-train-preprocess-128-512.ipynb  diabetic-test-preprocess.ipynb\r\n",
      "diabetic-gpu8-train-preprocess-256.ipynb      diabetic-train-preprocess.ipynb\r\n",
      "diabetic-model-gpu.ipynb\t\t      tensoboard_logs\r\n",
      "diabetic-model.ipynb\t\t\t      vgg_dr\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from medpy.io import load\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import scipy\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import imread\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Use_Model = \"VGGNet\" #\"ResNet\" or \"VGGNet\"\n",
    "image_size = 256\n",
    "num_epochs = 5\n",
    "# Send 'my_batch_size' number training data at a time to GPU, else GPU runs out of memory if entire x_train and y_train is sent. \n",
    "# May have to set it lower than 10 for 512 image size\n",
    "my_batch_size = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resize_128_dir = \"/home/ubuntu/final_data/train/128\"\n",
    "# Create directory for 128x128 resize\n",
    "try:\n",
    "    os.stat(resize_128_dir)\n",
    "except:\n",
    "    os.makedirs(resize_128_dir)\n",
    "    \n",
    "resize_256_dir = \"/home/ubuntu/final_data/train/256\"\n",
    "# Create directory for 256x256 resize\n",
    "try:\n",
    "    os.stat(resize_256_dir)\n",
    "except:\n",
    "    os.makedirs(resize_256_dir)\n",
    "\n",
    "resize_512_dir = \"/home/ubuntu/final_data/train/512\"\n",
    "# Create directory for 512x512 resize\n",
    "try:\n",
    "    os.stat(resize_512_dir)\n",
    "except:\n",
    "    os.makedirs(resize_512_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15_right</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16_left</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_right</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17_right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image  level\n",
       "0   10_left      0\n",
       "1  10_right      0\n",
       "2   13_left      0\n",
       "3  13_right      0\n",
       "4   15_left      1\n",
       "5  15_right      2\n",
       "6   16_left      4\n",
       "7  16_right      4\n",
       "8   17_left      0\n",
       "9  17_right      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the labels\n",
    "labels = pd.read_csv('/home/ubuntu/trainLabels.csv')\n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd2ca3f0d90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFfCAYAAAAS+IXqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC0mS1Mz/B2A4jSJ0gnj3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3d18c1450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class imbalance\n",
    "labels['level'].value_counts().plot(kind='bar')\n",
    "\n",
    "# Notes:\n",
    "# 5 labels present - 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21163, 256, 256, 3)\n",
      "(21163, 5)\n",
      "(13963, 256, 256, 3)\n",
      "(13963, 5)\n"
     ]
    }
   ],
   "source": [
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_val_list = []\n",
    "y_val_list = []\n",
    "\n",
    "train_percent = 0.6 # 60% train\n",
    "if image_size == 128:\n",
    "    Image_File_Path = resize_128_dir\n",
    "elif image_size == 256:\n",
    "    Image_File_Path = resize_256_dir\n",
    "else:\n",
    "    Image_File_Path = resize_512_dir\n",
    "        \n",
    "for index, row in labels.iterrows():\n",
    "    try:\n",
    "        img_data = np.fromfile(Image_File_Path + '/' + row['image'] + '.data', dtype='uint8', sep=\"\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    img_data = img_data.reshape([image_size, image_size, 3])\n",
    "    \n",
    "    rand_no = np.random.rand()\n",
    "    if rand_no <= train_percent:\n",
    "        x_list = x_train_list\n",
    "        y_list = y_train_list\n",
    "    else:\n",
    "        x_list = x_val_list\n",
    "        y_list = y_val_list\n",
    "        \n",
    "    if row['level'] == 0:\n",
    "        y_list.append([1, 0, 0, 0, 0])\n",
    "    elif row['level'] == 1:\n",
    "        y_list.append([0, 1, 0, 0, 0])\n",
    "    elif row['level'] == 2:\n",
    "        y_list.append([0, 0, 1, 0, 0])\n",
    "    elif row['level'] == 3:\n",
    "        y_list.append([0, 0, 0, 1, 0])\n",
    "    elif row['level'] == 4:\n",
    "        y_list.append([0, 0, 0, 0, 1])\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    x_list.append(img_data)\n",
    "\n",
    "x_train = np.array(x_train_list)\n",
    "y_train = np.array(y_train_list)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "\n",
    "x_val = np.array(x_val_list)\n",
    "y_val = np.array(y_val_list)\n",
    "\n",
    "print x_val.shape\n",
    "print y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " ..., \n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.734584\n",
       "2    0.152188\n",
       "1    0.068323\n",
       "3    0.025209\n",
       "4    0.019695\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.argmax(y_val, axis=1)).loc[:, 0].value_counts() / y_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for VGGNet\n",
    "def VGGNet_model(input_shape, num_classes, use_sgd=False, train_bottom=False):\n",
    "    model_vgg19_conv = VGG19(weights='imagenet', include_top=False)\n",
    "    model_vgg19_conv.summary()\n",
    "    \n",
    "    if not train_bottom: \n",
    "        for layer in model_vgg19_conv.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Create your own input format\n",
    "    input = Input(shape=input_shape, name = 'image_input')\n",
    "    \n",
    "    # Use the generated model \n",
    "    output_vgg19_conv = model_vgg19_conv(input)\n",
    "  \n",
    "    \n",
    "    # Add the fully-connected layers \n",
    "    x = Flatten(name='flatten')(output_vgg19_conv)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = Dense(1000, activation='relu', name='fc3')(x)\n",
    "\n",
    "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    # Create model \n",
    "    AD_model = Model(input=input, output=x)\n",
    "    AD_model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    if not use_sgd:\n",
    "        AD_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print \"using adam optimizer\"\n",
    "    else:\n",
    "        sgd = SGD(lr=0.01, momentum=0.8, decay=0.000001, nesterov=False)\n",
    "        AD_model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "        print \"using SGD optimizer\"\n",
    "    \n",
    "    return AD_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for resnet\n",
    "def ResNet_model(input_shape, num_classes, use_sgd=False):\n",
    "    model_resnet_conv = ResNet50(weights='imagenet', include_top=False)\n",
    "    \n",
    "#    for layer in model_vgg16_conv.layers:\n",
    "#        layer.trainable = False\n",
    "\n",
    "    # Create your own input format\n",
    "    input = Input(shape=input_shape, name = 'image_input')\n",
    "    \n",
    "    # Use the generated model \n",
    "    output_resnet_conv = model_resnet_conv(input)\n",
    "    \n",
    "    # Add the fully-connected layers \n",
    "    x = Flatten(name='flatten')(output_resnet_conv)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "\n",
    "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    # Create model \n",
    "    AD_model = Model(input=input, output=x)\n",
    "    AD_model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    if not use_sgd:\n",
    "        AD_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print \"using adam optimizer\"\n",
    "    else:\n",
    "        sgd = SGD(lr=0.01, momentum=0.8, decay=0.000001, nesterov=False)\n",
    "        AD_model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "        print \"using SGD optimizer\"\n",
    "    \n",
    "    return AD_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VGGNet\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg19 (Model)                multiple                  20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              134221824 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 5)                 5005      \n",
      "=================================================================\n",
      "Total params: 175,129,525\n",
      "Trainable params: 155,105,141\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "using SGD optimizer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg19 (Model)                multiple                  20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              134221824 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 5)                 5005      \n",
      "=================================================================\n",
      "Total params: 175,129,525\n",
      "Trainable params: 155,105,141\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"pr..., inputs=Tensor(\"im...)`\n"
     ]
    }
   ],
   "source": [
    "if Use_Model == \"ResNet\":\n",
    "    print \"Using ResNet\"\n",
    "    my_model = ResNet_model((image_size,image_size,3), 5, use_sgd=True)\n",
    "else:\n",
    "    print \"Using VGGNet\"\n",
    "    my_model= VGGNet_model((image_size,image_size,3), 5, use_sgd=True, train_bottom=False)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard_dir = \"/home/ubuntu/tb_logs/\" + str(image_size) + \"_\" + str(Use_Model)\n",
    "# Create directory\n",
    "try:\n",
    "    os.stat(tensorboard_dir)\n",
    "except:\n",
    "    os.makedirs(tensorboard_dir)\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_dir, \n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True, \n",
    "                          write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create model checkpoints\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "vgg_tboard_dr = TensorBoard(log_dir='tensoboard_logs/vgg_dr')\n",
    "vgg_model_check_dr = ModelCheckpoint(filepath='vgg_dr', monitor='val_acc', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(X, y):\n",
    "    n, p = X.shape[0:2]\n",
    "    for i in range(n):\n",
    "        yield X[i][np.newaxis], y[i][np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train = x_train.shape[0]\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "data_aug = ImageDataGenerator(width_shift_range=0.25, height_shift_range=0.25, horizontal_flip=True, zoom_range=.25)\n",
    "data_aug.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "165/165 [==============================] - 127s - loss: 1.7021 - acc: 0.8938   \n",
      "Epoch 2/30\n",
      "165/165 [==============================] - 127s - loss: 1.8304 - acc: 0.8858   \n",
      "Epoch 3/30\n",
      "165/165 [==============================] - 127s - loss: 1.6982 - acc: 0.8941   \n",
      "Epoch 4/30\n",
      "165/165 [==============================] - 127s - loss: 1.7954 - acc: 0.8880   \n",
      "Epoch 5/30\n",
      "165/165 [==============================] - 127s - loss: 1.6166 - acc: 0.8992   \n",
      "Epoch 6/30\n",
      "165/165 [==============================] - 127s - loss: 1.6166 - acc: 0.8992   \n",
      "Epoch 7/30\n",
      "165/165 [==============================] - 127s - loss: 1.6671 - acc: 0.8960   \n",
      "Epoch 8/30\n",
      "  4/165 [..............................] - ETA: 124s - loss: 1.1221 - acc: 0.9300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-791a2e8e031e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m my_model.fit_generator(data_aug.flow(x_train, y_train, batch_size=10), \n\u001b[1;32m      2\u001b[0m                        \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                        callbacks=[vgg_tboard_dr, vgg_model_check_dr])\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_model.fit_generator(data_aug.flow(x_train, y_train, batch_size=10), \n",
    "                       steps_per_epoch=n_train/batch_size, epochs=30,\n",
    "                       callbacks=[vgg_tboard_dr, vgg_model_check_dr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_model = my_model.predict(x_train, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13963/13963 [==============================] - 916s   \n"
     ]
    }
   ],
   "source": [
    "my_model_val_predict = my_model.predict(x_val, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>954</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4\n",
       "0  10257  0  0  0  0\n",
       "1    954  0  0  0  0\n",
       "2   2125  0  0  0  0\n",
       "3    352  0  0  0  0\n",
       "4    275  0  0  0  0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(np.argmax(y_val, axis=1), np.argmax(my_model_val_predict, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "for epochs in range(0,num_epochs):\n",
    "    i = 0\n",
    "    start_time = time.time()\n",
    "    while i < (x_train.shape[0] - my_batch_size):\n",
    "        x_temp = x_train[i:i + my_batch_size]\n",
    "        y_temp = y_train[i:i + my_batch_size]\n",
    "        #print \"i = \", i\n",
    "        my_model.fit(x_temp, y_temp, \n",
    "                     epochs=1, \n",
    "                     batch_size=my_batch_size, \n",
    "                     callbacks=[tensorboard],  # tensorboard callback, optional\n",
    "                     verbose=0)\n",
    "        i = i + 10\n",
    "    print \"epoch = \", epochs, \"duration = \", time.time() - start_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14070/14070 [==============================] - 459s   \n",
      "CNN Error: 9.56%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model using validation set\n",
    "scores = my_model.evaluate(x_val, y_val, verbose=1)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15825, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read test data and create array\n",
    "x_test_list = []\n",
    "\n",
    "if image_size == 128:\n",
    "    Test_File_Path = \"/home/ubuntu/final_data/test/128/\"\n",
    "elif image_size == 256:\n",
    "    Test_File_Path = \"/home/ubuntu/final_data/test/256/\"\n",
    "else:\n",
    "    Test_File_Path = \"/home/ubuntu/final_data/test/512/\"\n",
    "\n",
    "for root, dirs, files in os.walk(Test_File_Path):\n",
    "    for fileName in files:\n",
    "        img_data = np.fromfile(Test_File_Path + fileName, dtype='uint8', sep=\"\")\n",
    "        img_data = img_data.reshape([image_size, image_size, 3])\n",
    "        x_test_list.append(img_data)\n",
    "\n",
    "x_test = np.array(x_test_list)\n",
    "print x_test.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15825/15825 [==============================] - 518s   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.53209877e-01,   2.90108826e-02,   1.65042654e-02,\n",
       "          6.66079111e-04,   6.08820585e-04],\n",
       "       [  9.24454868e-01,   3.96133959e-02,   2.92053055e-02,\n",
       "          2.93742237e-03,   3.78899020e-03],\n",
       "       [  8.92437238e-04,   4.71559964e-04,   7.89436162e-01,\n",
       "          1.96511209e-01,   1.26886275e-02],\n",
       "       ..., \n",
       "       [  9.90668595e-01,   3.01484182e-03,   5.51996706e-03,\n",
       "          1.80064948e-04,   6.16424310e-04],\n",
       "       [  9.91068065e-01,   3.08298087e-03,   5.62813459e-03,\n",
       "          9.93706562e-05,   1.21509271e-04],\n",
       "       [  9.58285272e-01,   2.86733266e-02,   1.07360734e-02,\n",
       "          8.89301184e-04,   1.41594710e-03]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict using model\n",
    "my_model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
